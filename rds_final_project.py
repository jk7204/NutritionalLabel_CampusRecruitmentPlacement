# -*- coding: utf-8 -*-
"""RDS Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-Nvox5GBnoT_eWrhqn5IS7wgfIvfVD9

# **Responsible Data Science Final Project**
"""

# Commented out IPython magic to ensure Python compatibility.
! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle kernels output atishadhikari/placement-dataanalysis-classification-regression -p /path/to/dest
! pip install lime
# %pip install numpy matplotlib seaborn
!pip install numba==0.48
!pip install aif360==0.2.2
!python -m pip install BlackBoxAuditing
!pip install tensorflow==1.13.1

"""Functions for LIME"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import sklearn.model_selection
import sklearn.metrics
import sklearn.datasets
import sklearn.ensemble
import sklearn.preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from IPython.display import Markdown, display
import warnings
import lime
import lime.lime_tabular
from lime import submodular_pick
import xgboost
from xgboost import plot_importance
import json
from collections import OrderedDict

import os
import matplotlib.pyplot as plt
from matplotlib import rcParams
from IPython.display import display
import numpy as np
import pandas as pd
import seaborn as sns
pd.options.display.max_columns = None
np.random.seed(1)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
rcParams['figure.figsize'] = [15, 5]

"""# **Exploratory Data Analysis**"""

inspections = pd.read_csv("Placement_Data_Full_Class.csv")

inspections.head()

inspections.drop("sl_no", axis=1, inplace=True)

print(inspections.columns)
print(inspections.dtypes)

cat_cols = inspections.select_dtypes(include=object).columns
print(cat_cols)
for column in cat_cols:
     print("\n" + column)
     print(inspections[column].value_counts())

num_cols = inspections.select_dtypes(include=np.number).columns
print(num_cols)
for column in num_cols:
     print("\n" + column)
     print(inspections[column].describe())

matrix = inspections.corr()
plt.figure(figsize=(16,12))

_ = sns.heatmap(matrix)

"""# **Explore features that may affect placement**

Gender
"""

sns.countplot("gender", hue="status", data=inspections)
plt.show()

"""Secondary Education Percentage and Board of Education"""

sns.kdeplot(inspections.ssc_p[inspections.status=="Placed"])
sns.kdeplot(inspections.ssc_p[inspections.status=="Not Placed"])
plt.legend(["Placed", "Not Placed"])
plt.xlabel("Secondary Education Percentage")
plt.show()

sns.countplot("ssc_b", hue="status", data=inspections)
plt.show()

"""Higher Secondary Education Percentage, Board of Education and Specialization"""

sns.kdeplot(inspections.hsc_p[inspections.status=="Placed"])
sns.kdeplot(inspections.hsc_p[inspections.status=="Not Placed"])
plt.legend(["Placed", "Not Placed"])
plt.xlabel("Higher Secondary Education Percentage")
plt.show()

sns.countplot("hsc_b", hue="status", data=inspections)
plt.show()

sns.countplot("hsc_s", hue="status", data=inspections)
plt.show()

"""Undergraduate Degree Percentage and Field"""

sns.kdeplot(inspections.degree_p[inspections.status=="Placed"])
sns.kdeplot(inspections.degree_p[inspections.status=="Not Placed"])
plt.legend(["Placed", "Not Placed"])
plt.xlabel("Under Graduate Percentage")
plt.show()

sns.countplot("degree_t", hue="status", data=inspections)
plt.show()

"""Work Experience"""

sns.countplot("workex", hue="status", data=inspections)
plt.show()

"""Employability Test Percentage"""

sns.kdeplot(inspections.etest_p[inspections.status=="Placed"])
sns.kdeplot(inspections.etest_p[inspections.status=="Not Placed"])
plt.legend(["Placed", "Not Placed"])
plt.xlabel("Employability test percentage")
plt.show()

"""Postgraduate MBA Specialization"""

sns.countplot("specialisation", hue="status", data=inspections)
plt.show()

"""MBA Percentage"""

sns.boxplot("mba_p", "status", data=inspections)
plt.show()

"""# **Explore features that may affect salary offered**

Gender
"""

sns.kdeplot(inspections.salary[inspections.gender=="M"])
sns.kdeplot(inspections.salary[inspections.gender=="F"])
plt.legend(["Male", "Female"])
plt.xlabel("Salary (100k)")
plt.show()

plt.figure(figsize =(18,6))
sns.boxplot("salary", "gender", data=inspections)
plt.show()

"""Secondary Education Percentage and Board of Education"""

sns.lineplot("ssc_p", "salary", hue="ssc_b", data=inspections)
plt.show()

sns.boxplot("salary", "ssc_b", data=inspections)
plt.show()

"""Higher Secondary Education Percentage, Board of Education and Specialization"""

sns.boxplot("salary", "hsc_b", data=inspections)
plt.show()

sns.lineplot("hsc_p", "salary", hue="hsc_b", data=inspections)
plt.show()

sns.boxplot("salary", "hsc_s", data=inspections)
plt.show()

sns.lineplot("hsc_p", "salary", hue="hsc_s", data=inspections)
plt.show()

"""Undergraduate Degree Percentage and Field"""

sns.boxplot("salary", "degree_t", data=inspections)
plt.show()

sns.lineplot("degree_p", "salary", hue="degree_t", data=inspections)
plt.show()

"""Work Experience"""

sns.boxplot("salary", "workex", data=inspections)
plt.show()

"""Employability Test Percentage"""

sns.lineplot("etest_p", "salary", data=inspections)
plt.show()

"""Postgraduate MBA Specialization"""

sns.boxplot("salary", "specialisation", data=inspections)
plt.show()

"""MBA Percentage"""

sns.lineplot("mba_p", "salary", data=inspections)
plt.show()

"""Feature Selection and Encoding"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""# **Data Pre-Processing**"""

inspections.drop(['ssc_b','hsc_b'], axis=1, inplace=True)

"""**Featuer Encoding**"""

data.dtypes

inspections["gender"] = inspections.gender.map({"M":0,"F":1})
inspections["hsc_s"] = inspections.hsc_s.map({"Commerce":0,"Science":1,"Arts":2})
inspections["degree_t"] = inspections.degree_t.map({"Comm&Mgmt":0,"Sci&Tech":1, "Others":2})
inspections["workex"] = inspections.workex.map({"No":0, "Yes":1})
inspections["status"] = inspections.status.map({"Not Placed":0, "Placed":1})
inspections["specialisation"] = inspections.specialisation.map({"Mkt&HR":0, "Mkt&Fin":1})

"""# **LIME for Placement Prediction**"""

data_placement = inspections.copy()
data_placement.drop(['salary'], axis=1, inplace=True)
colnames = ['gender', 'ssc_p', 'hsc_p', 'hsc_s', 'degree_p', 'degree_t', 'workex','etest_p', 'specialisation', 'mba_p',]
feature_names = data_placement.columns[:-1]
data_placement.head()

sns.pairplot(data=data_placement, vars=('ssc_p','hsc_p','degree_p','etest_p','mba_p'), hue='status' )

labels = data_placement.iloc[:,-1]
le= sklearn.preprocessing.LabelEncoder()
le.fit(labels)
labels = le.transform(labels)
class_names = le.classes_
data = data_placement.iloc[:,:-1]
le_label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Class names: ", class_names)
print("Label mapping: ", le_label_mapping)

print(data.dtypes)
# Get a list of which variables are categorical
categorical_features  = [i for i in range(len(data.dtypes)) if data.dtypes[i]=='int64']
print("Indices of categorical features: ", categorical_features)

categorical_names = {}
for feature in categorical_features:
    print("Feature: ", feature)
    # Use label encoder to map categories to numbers
    le = sklearn.preprocessing.LabelEncoder()
    le.fit(data.iloc[:, feature])
    # Replace the categories with corresponding numbers in the original data
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    # Store and print the mappings for reference later
    categorical_names[feature] = le.classes_
    print(categorical_names[feature])
    print("==================================================")

encoder = ColumnTransformer(transformers=[('get_dummies', OneHotEncoder(handle_unknown="ignore"), categorical_features)], remainder='passthrough')
encoder = encoder.fit(data)

train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.7)
print("Train shape: ", train.shape)
print("Test shape: ", test.shape)

"""# **Decision Tree Classifier**"""

dtree = DecisionTreeClassifier(criterion='entropy')
dtree.fit(encoder.transform(train), labels_train)
labels_pred = dtree.predict(encoder.transform(test))

print("Test set accuracy: ", sklearn.metrics.accuracy_score(labels_test, labels_pred))

print(sklearn.metrics.classification_report(labels_test, labels_pred))

explainer = lime.lime_tabular.LimeTabularExplainer(train.values, 
                                                   feature_names=feature_names,
                                                   class_names=class_names,
                                                   categorical_features=categorical_features, 
                                                   categorical_names=categorical_names)

predict_fn = lambda x: dtree.predict_proba(encoder.transform(x)).astype(float)

i = 3
print('Actual class: ', labels_test[i])
# Get explanation
exp = explainer.explain_instance((test.values[i]), predict_fn, num_features=5)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig = exp.as_pyplot_figure()

exp.show_in_notebook(show_all=True)

sp_obj = submodular_pick.SubmodularPick(explainer, train.values, predict_fn, sample_size=10, 
                                        num_features=5, num_exps_desired=5)

for ind in sp_obj.V:
    exp = explainer.explain_instance(test.values[ind], predict_fn, num_features=5)
    print("Actual class: ", labels_test[ind])
    exp.show_in_notebook(show_all=False)
    print("==========================")

"""# Random Forest Classifier"""

rftree = RandomForestClassifier(n_estimators=100)
rftree.fit(encoder.transform(train), labels_train)
labels_pred = rftree.predict(encoder.transform(test))

print("Test set accuracy: ", sklearn.metrics.accuracy_score(labels_test, labels_pred))

print(sklearn.metrics.classification_report(labels_test, labels_pred))

predict_fn = lambda x: rftree.predict_proba(encoder.transform(x)).astype(float)

i = 3
print('Actual class: ', labels_test[i])
# Get explanation
exp = explainer.explain_instance((test.values[i]), predict_fn, num_features=5)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig = exp.as_pyplot_figure()

exp.show_in_notebook(show_all=True)

sp_obj = submodular_pick.SubmodularPick(explainer, train.values, predict_fn, sample_size=10, 
                                        num_features=5, num_exps_desired=5)

for ind in sp_obj.V:
    exp = explainer.explain_instance(test.values[ind], predict_fn, num_features=5)
    print("Actual class: ", labels_test[ind])
    exp.show_in_notebook(show_all=False)
    print("==========================")

"""# **Logistic Regression Model**"""

logistic_reg = LogisticRegression()
logistic_reg.fit(train, labels_train)
labels_pred = logistic_reg.predict(test)

print("Test set accuracy: ", sklearn.metrics.accuracy_score(labels_test, labels_pred))

print(sklearn.metrics.classification_report(labels_test, labels_pred))

#i = 3
#print(len(test.values[i]))
#print('Actual class: ', labels_test[i])
# Get explanation
#exp = explainer.explain_instance((test.values[i]), logistic_reg.predict_proba , num_features=5)

#explanation.show_in_notebook()

#%matplotlib inline
#fig = exp.as_pyplot_figure()

sp_obj = submodular_pick.SubmodularPick(explainer, train.values, predict_fn, sample_size=10, 
                                        num_features=5, num_exps_desired=5)

for ind in sp_obj.V:
    exp = explainer.explain_instance(test.values[ind], logistic_reg.predict_proba, num_features=5)
    print("Actual class: ", labels_test[ind])
    exp.show_in_notebook(show_all=False)
    print("==========================")

"""# **LIME for Salary Prediction**"""

from sklearn.preprocessing import MinMaxScaler

data_salary = inspections.copy()
data_salary.drop(['status'], axis=1, inplace=True)
data_salary.dropna(inplace=True)
colnames = ['gender', 'ssc_p', 'hsc_p', 'hsc_s', 'degree_p', 'degree_t', 'workex','etest_p', 'specialisation', 'mba_p',]
feature_names = data_salary.columns[:-1]
data_salary.head()

sns.pairplot(data=data_salary, vars=('ssc_p','hsc_p','degree_p','etest_p','mba_p'), hue='salary' )

labels = data_salary.iloc[:,-1]
le= sklearn.preprocessing.LabelEncoder()
le.fit(labels)
labels = le.transform(labels)
class_names = le.classes_
data = data_salary.iloc[:,:-1]
le_label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Class names: ", class_names)
print("Label mapping: ", le_label_mapping)

print(data.dtypes)
# Get a list of which variables are categorical
categorical_features  = [i for i in range(len(data.dtypes)) if data.dtypes[i]=='object']
print("Indices of categorical features: ", categorical_features)

categorical_names = {}
for feature in categorical_features:
    print("Feature: ", feature)
    # Use label encoder to map categories to numbers
    le = sklearn.preprocessing.LabelEncoder()
    le.fit(data_scaled.iloc[:, feature])
    # Replace the categories with corresponding numbers in the original data
    data_scaled.iloc[:, feature] = le.transform(data_scaled.iloc[:, feature])
    # Store and print the mappings for reference later
    categorical_names[feature] = le.classes_
    print(categorical_names[feature])
    print("==================================================")

encoder = ColumnTransformer(transformers=[('get_dummies', OneHotEncoder(), categorical_features)], remainder='passthrough')
encoder = encoder.fit(data_scaled)

train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data_scaled, labels, train_size=0.7)
print("Train shape: ", train.shape)
print("Test shape: ", test.shape)

"""# **Linear Regression Model**"""

lr = LinearRegression()
lr.fit(encoder.transform(train), labels_train)
labels_pred = lr.predict(encoder.transform(test))

print("Test R^2 Score  : ", lr.score(test, labels_test))

explainer = lime.lime_tabular.LimeTabularExplainer(train.values, mode="regression",
                                                   feature_names=feature_names,
                                                   class_names=class_names,
                                                   categorical_features=categorical_features, 
                                                   categorical_names=categorical_names)

#i = 3

#print("Actual :     ", labels_test[i])

#exp = explainer.explain_instance(test[i], lr.predict, num_features=5)

#%matplotlib inline
#fig = exp.as_pyplot_figure()

#exp.show_in_notebook(show_all=True)

sp_obj = submodular_pick.SubmodularPick(explainer, train.values, lr.predict, sample_size=10, 
                                        num_features=5, num_exps_desired=5)

for ind in sp_obj.V:
    exp = explainer.explain_instance(test.values[ind], lr.predict, num_features=5)
    print("Actual class: ", labels_test[ind])
    exp.show_in_notebook(show_all=False)
    print("==========================")